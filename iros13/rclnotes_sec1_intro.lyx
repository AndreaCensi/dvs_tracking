#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass IEEEtran
\begin_preamble
\input{tex/preamble.tex}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\use_refstyle 0
\branch report
\selected 0
\filename_suffix 0
\color #000000
\end_branch
\branch conf
\selected 0
\filename_suffix 0
\color #000000
\end_branch
\branch AC
\selected 1
\filename_suffix 0
\color #682743
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
LARGE
\backslash
bf
\end_layout

\end_inset

 Low-latency localization by Active LED Markers tracking 
\begin_inset Newline newline
\end_inset

using a Dynamic Vision Sensor
\end_layout

\begin_layout Author
Andrea Censi
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mythanks
\end_layout

\end_inset

, Jonas Strubel, Christian Brandli, Tobi Delbruck, Davide Scaramuzza
\end_layout

\begin_layout Abstract
At the current state of the art, the agility of an autonomous flying robot
 is limited by its sensing pipeline, because the relatively high latency
 and low sampling frequency limit the aggressiveness of the control strategies
 that can be implemented.
 To obtain more agile robots, we need faster sensing pipelines.
 A Dynamic Vision Sensor (DVS) is a very different sensor than a normal
 CMOS camera: rather than providing discrete frames like a CMOS camera,
 the sensor output is a sequence of asynchronous timestamped events each
 describing a change in the perceived brightness at a single pixel.
 The latency of such sensors can be measured in the microseconds, thus offering
 the theoretical possibility of creating a sensing pipeline whose latency
 is negligible compared to the dynamics of the platform.
 However, to use these sensors we must rethink the way we interpret visual
 data.
 This paper presents a method for low-latency pose tracking using a DVS
 and Active Led Markers (ALMs), which are LEDs blinking at high frequency
 (>1
\begin_inset space ~
\end_inset

KHz).
 The sensor's time resolution allows distinguishing different frequencies,
 thus avoiding the need for data association.
 This approach is compared to traditional pose tracking based on a CMOS
 camera.
 The DVS performance is not affected by fast motion, unlike the CMOS camera,
 which suffers from motion blur.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Autonomous micro helicopters will soon play a major role in tasks such search
 and rescue, environment monitoring, security surveillance, inspection.
 A key problem in aerial-vehicle navigation is pose stabilization and trajectory
 control in six degrees of freedom using onboard sensors.
 Experiments with prototype systems that use an external motion-tracking
 systems have shown that the platform themselves allow extreme maneuverability
 if the localization problem can be assumed to be solved
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Lupashin2012"

\end_inset

.
 However, such extreme performance is not attainable, not even in principle,
 with traditional robotic sensors, such as CMOS cameras
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Weiss2011"

\end_inset

 or laser rangefinders
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Shen2011"

\end_inset

.
\end_layout

\begin_layout Standard
The agility of an autonomous flying robot is limited by the speed of the
 sensing pipeline.
 More precisely, 
\begin_inset Quotes eld
\end_inset

speed
\begin_inset Quotes erd
\end_inset

 can be quantified in observations
\emph on
 frequency
\emph default
 and 
\emph on
latency
\emph default
 (
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Discretization-and-latency"

\end_inset

).
 For a sensing pipeline based on a CMOS camera, the observations are captured
 at a frequency on the order of 15--30
\begin_inset space ~
\end_inset

Hz and the total latency of the pipeline, including both image acquisition
 and image processing using common visual odometry approaches is in the
 order of 50--250
\begin_inset space ~
\end_inset

ms
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Weiss2011"

\end_inset

.
 To obtain more agile systems, we need to use faster sensors and low-latency
 processing.
\end_layout

\begin_layout Standard
This paper presents a method for pose tracking based on the use of a Dynamic
 Vision Sensor (DVS).
 The main difference between a DVS and a normal CMOS camera is that the
 DVS output is a stream of 
\emph on
events
\emph default
 that encode 
\emph on
changes
\emph default
 in the brightness.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
This is called an address-event representation (AER); 
\end_layout

\end_inset

Each event encodes the location of the change, whether there was a positive
 or negative change in brightness.
 The timestamp has a resolution in the order of 1
\begin_inset space ~
\end_inset


\begin_inset Formula $\mu$
\end_inset

s.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Intuitively, the events generated can be interpreted as the sign of the
 derivative of the luminance, but this is just an idealization (
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:The-DVS-camera"

\end_inset

 describes the principles behind the device).
\end_layout

\end_inset

 These events are not unlike spikes in a biological visual system; however,
 while retinal ganglion cells show latencies of around 200
\begin_inset space ~
\end_inset

ms, the DVS chip has a latency of 
\begin_inset Formula $15$
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $\mu$
\end_inset

s.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
sin fact, 
\begin_inset Quotes eld
\end_inset

silicon retina
\begin_inset Quotes erd
\end_inset

 is a nickname for the DVS.
 But the DVS circuits are much faster than a slow neuron: events are generated
 with a latency of only 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
xxx
\end_layout

\end_inset

 Âµs.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Theoretically, using a DVS we could obtain sensing pipelines with a negligible
 latency compared to dynamics of the platform.
 We are a few years to the goal, however.
 On the hardware side, the current version of the DVS that is available
 commercially has a few limitations, such as the limited resolution of 
\begin_inset Formula $128\times128$
\end_inset

 pixels, which is the limiting factor for robotics applications.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
, and it is too heavy to be attached on current agile drones.
 
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
These problems
\end_layout

\end_inset

 It is projected that in a couple of generations the technology will progress
 to have comparable resolution with traditional cameras.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/slides/latency1.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Traditional architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/slides/latency2.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Low latency, event-based architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Discretization-and-latency"

\end_inset

To improve the agility of autonomous flying robots, we need to improve the
 total latency of the sensing pipeline.
 Using a device like a Dynamic Vision Sensor (DVS) we can theoretically
 obtain a sensing pipeline which has microsecond latency.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
On the theory side, to take full advantage of the sensor capability we need
 to rethink completely the way we design robotic sensing pipelines.
 In principle, it is possible to integrate the events of a DVS camera to
 simulate a regular CMOS frame, and then adapt techniques from standard
 image processing.
 However, that is not desirable, because it would result in the same latency
 of a regular camera.
 Ideally, to have the lowest latency for the sensing pipeline, one would
 want each single event to be be reflected in a small but instantaneous
 change in the commands given to the actuators.
 Therefore, our approach is to consider methods that make use the information
 contained in each single event.
\end_layout

\begin_layout Standard
The pose-tracking method presented in this paper is baed on using Active
 LED Markers (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ALMs
\end_layout

\end_inset

).
 These are infrared LEDs which are controlled to blink at given known high
 frequencies, in the range of 1--2
\begin_inset space ~
\end_inset


\begin_inset Formula $\,\mbox{kHz}$
\end_inset

.
 The DVS is fast enough to be able to distinguish different blinking frequencies
, so that, with proper processing, it is possible to also uniquely assign
 an observable identity to each marker.
 We envision that this system could be used for inter-robot localization
 for high-speed acrobatic maneuvers, or that, in applications such as rescue
 robotics, these markers could be left in the environment to facilitate
 cooperative mapping.
\end_layout

\begin_layout Standard
One approach to using the DVS data is to cluster the events in order to
 find spatio-temporal features, like points or lines, that are then tracked
 through time
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "delbruck07fast,conradt09pencil,Matthias"

\end_inset

.
 This approach works well when the camera is static, because the output
 is spatiotemporally sparse.
 The algorithm presented in this paper uses a different approach.
 We found out that mounting a DVS camera on a flying robot creates a new
 set of challenges.
 Because of the apparent motion of the environment, the events are not spatiotem
porally sparse anymore.
 Moreover, while in controlled conditions the DVS camera parameters can
 be tuned to obtain the best performance, a robot must be able to work in
 a wider range of environmental conditions and be robust to interferences.
 To achieve this robustness we have developed an approach that sacrifices
 some latency to be more robust to noise and unmodeled phenomena.
 We accumulate the events perceived in thin slices of times corresponding
 to the blinking frequency (
\begin_inset Formula $1$
\end_inset


\begin_inset space ~
\end_inset

ms slice for 1
\begin_inset space ~
\end_inset

kHz data).
 This allows to do detection of the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ALMs
\end_layout

\end_inset

 position in image space.
 On top of this, we use a particle filter for tracking the position in image
 space of each detection, and a disambiguation stage to obtain coherent
 hypotheses on the joint position of the markers.
 Finally, we reconstruct the pose using a standard approach to rigid reconstruct
ion.
\end_layout

\begin_layout Standard
Our method is evaluated in the application of tracking the pose of a drone
 during an aggressive maneuver (a flip), and it is compared to a more traditiona
l approach, based on using a CMOS camera and a feature-based visual odometry
 method.
 Experiments show that our method, with a latency of 
\begin_inset Formula $1$
\end_inset


\begin_inset space ~
\end_inset

ms, is able to reacquire tracking instantaneously regardless of the fast
 motion, while the CMOS data is unusable for visual odometry because it
 is corrupted by motion blur.
 We evaluate the reconstruction accuracy using an OptiTrack system and find
 values that are compatible with the low spatial resolution (128
\begin_inset Formula $\times$
\end_inset

128) of the DVS, which proves to be the current limitation of this approach.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
As this utilization of asynchronous vision is a rather novel approach in
 robotics it would be interesting to use this camera in other navigational
 tasks in future projects, for example for visual SLAM on autonomous ground
 vehicles.
 While small image features as used in SIFT 
\begin_inset CommandInset citation
LatexCommand cite
key "SIFT"

\end_inset

 might be difficult to use due to resolution line feature extraction, as
 describe in 
\begin_inset CommandInset citation
LatexCommand cite
key "LineTracking"

\end_inset

, could be a feasible approach.
 Additionally, as edges are the natural source for DVS events it would have
 an advantage in terms of processing compared to normal cameras.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Software, datasets, and videos illustrating the method are available at
 the website 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
myurl
\end_layout

\end_inset

.
\end_layout

\end_body
\end_document
