#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass IEEEtran
\begin_preamble
\input{tex/preamble.tex}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\use_refstyle 0
\branch report
\selected 0
\filename_suffix 0
\color #000000
\end_branch
\branch conf
\selected 0
\filename_suffix 0
\color #000000
\end_branch
\branch AC
\selected 1
\filename_suffix 0
\color #682743
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
LARGE
\backslash
bf
\end_layout

\end_inset

 Low-latency localization by Active LED Markers tracking 
\begin_inset Newline newline
\end_inset

using a Dynamic Vision Sensor
\end_layout

\begin_layout Author
Andrea Censi
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mythanks
\end_layout

\end_inset

, Jonas Strubel, Christian Brandli, Tobi Delbruck, Davide Scaramuzza
\end_layout

\begin_layout Abstract
At the current state of the art, the agility of an autonomous flying robot
 is limited by the speed of its sensing pipeline, as the relatively high
 latency and low sampling frequency limit the aggressiveness of the control
 strategies that can be implemented.
 Dynamic Vision Sensors (DVS) encode changes in the perceived brightness
 using an address-event representation.
 The latency of such sensors can be measured in the microseconds, thus offering
 the theoretical possibility of creating a sensing pipeline whose latency
 is negligible compared to the dynamics of the platform.
 However, to use these sensors we must rethink the way we interpret visual
 data.
 We present an approach to low-latency pose tracking using a DVS and Active
 Led Markers (ALMs), which are LEDs blinking at high frequency (>1
\begin_inset space ~
\end_inset

KHz).
 The DVS time resolution is able to distinguish different frequencies, thus
 avoiding the need for data association.
 We compare the DVS approach to traditional tracking using a CMOS camera,
 and we show that the DVS performance is not affected by fast motion, unlike
 the CMOS camera, which suffers from motion blur.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Autonomous micro helicopters will soon play a major role in tasks like search
 and rescue, environment monitoring, security surveillance, inspection,
 etc.
 A key problem in aerial-vehicle navigation is the stabilization and control
 in six degrees of freedom.
 Today's systems handle well the attitude control.
 However, without a position control, they are prone to drift over time.
 In GPS-denied environments, this can be solved using onboard sensors, such
 as cameras
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Weiss2011"

\end_inset

 or laser rangefinders
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Shen2011"

\end_inset

; however, the achievable vehicle maneuvers are still too slow compared
 to those attainable with off-board motion-tracking systems (e.g., Vicon)
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Lupashin2012"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement b
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/slides/latency1.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Traditional architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/slides/latency2.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Low latency, event-based architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Discretization-and-latency"

\end_inset

To improve the agility of autonomous flying robots, we need to improve the
 total latency of the sensing pipeline.
 Using a device like a Dynamic Vision Sensor (DVS) we can theoretically
 obtain a sensing pipeline which has microsecond latency.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The agility of an autonomous flying robot is limited by the speed of the
 sensing pipeline.
 More precisely, 
\begin_inset Quotes eld
\end_inset

speed
\begin_inset Quotes erd
\end_inset

 can be quantified in 
\emph on
observations frequency
\emph default
 and 
\emph on
latency
\emph default
 (
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Discretization-and-latency"

\end_inset

).
 In current state-of-the art autonomous navigation applications
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Weiss2011"

\end_inset

 cameras give observations with frequency of 15--30
\begin_inset space ~
\end_inset

Hz and the total latency, from acquiring the images to processing them,
 is in the order of 50--250
\begin_inset space ~
\end_inset

ms.
 To obtain more agile systems, we need to use faster sensors and low-latency
 processing.
\end_layout

\begin_layout Standard
In this paper, we consider the use of a Dynamic Vision Sensor (DVS) for
 pose tracking.
 The main difference between a DVS and a normal CMOS camera is that the
 DVS output is a stream of 
\emph on
events
\emph default
 that encode 
\emph on
changes
\emph default
 in the brightness.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
This is called an address-event representation (AER); 
\end_layout

\end_inset

Each event encodes the location of the change, whether there was a positive
 or negative change in brightness, and has a 1
\begin_inset space ~
\end_inset


\begin_inset Formula $\mu$
\end_inset

s timestamp.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Intuitively, the events generated can be interpreted as the sign of the
 derivative of the luminance, but this is just an idealization (
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:The-DVS-camera"

\end_inset

 describes the principles behind the device).
\end_layout

\end_inset

 These events are not unlike spikes in a biological visual system; however,
 while retinal ganglion cells show latencies of around 200
\begin_inset space ~
\end_inset

ms, the DVS chip has a latency of 
\begin_inset Formula $15\,\mu s$
\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
sin fact, 
\begin_inset Quotes eld
\end_inset

silicon retina
\begin_inset Quotes erd
\end_inset

 is a nickname for the DVS.
 But the DVS circuits are much faster than a slow neuron: events are generated
 with a latency of only 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
xxx
\end_layout

\end_inset

 Âµs.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Theoretically, using a DVS we could obtain sensing pipelines with a negligible
 latency compared to dynamics of the platform.
 We are a few years to the goal, however.
 On the hardware side, the DVS camera, though currently available commercially,
 has a few limitations, such as the limited resolution (
\begin_inset Formula $128\times128$
\end_inset

 pixels), which 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
, and it is too heavy to be attached on current agile drones.
 
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
These problems
\end_layout

\end_inset

 will be increased in the next generation of prototypes currently in development.
 On the software side, to take full advantage of this data we need to rethink
 completely the way we design robotic sensing pipelines.
 It is possible to integrate the events of a DVS camera to simulate a regular
 CMOS frame, on which to do standard image processing, however, that is
 not desirable, because that would give the same latency of a regular camera.
 Ideally, to have the lowest latency for the sensing pipeline, one would
 want each single event to be be reflected in a small but instantaneous
 change in the commands given to the actuators.
 Therefore, we consider approaches that possibly use the information contained
 in each single event.
\end_layout

\begin_layout Standard
In this paper, we consider the application of pose tracking based on Active
 LED Markers (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ALMs
\end_layout

\end_inset

), which are infrared LEDs blinking at  high frequency (
\begin_inset Formula $>1\,\mbox{kHz}$
\end_inset

).
 The DVS is fast enough to be able to distinguish different blinking frequencies
, so that we can also uniquely assign an observable identity to each marker.
 We envision that this system could be used for inter-robot localization
 for high-speed acrobatic maneuvers, or that, in applications such as rescue
 robotics, these markers could be left in the environment to facilitate
 cooperative mapping.
\end_layout

\begin_layout Standard
One approach to using the DVS data is to cluster the events in order to
 find spatio-temporal features, like points or lines, that are then tracked
 through time
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "delbruck07fast,conradt09pencil,Matthias"

\end_inset

.
 This approach works well when the camera is static, because the output
 is spatiotemporally sparse.
 
\end_layout

\begin_layout Standard
The algorithm presented in this paper uses a different approach.
 We found out that mounting a DVS camera on a flying robot creates a new
 set of challenges.
 Because of the apparent motion of the environment, the events are not spatiotem
porally sparse anymore.
 Moreover, while in controlled conditions the DVS camera parameters can
 be tuned to obtain the best performance, a robot must be able to work in
 a wider range of environmental conditions and be robust to interferences.
 To achieve this robustness we have developed an approach that sacrifices
 some latency to be more robust to noise and unmodeled phenomena.
 We accumulate the events perceived in thin slices of times corresponding
 to the blinking frequency (
\begin_inset Formula $1$
\end_inset


\begin_inset space ~
\end_inset

ms slice for 1
\begin_inset space ~
\end_inset

kHz data).
 This allows to do detection of the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ALMs
\end_layout

\end_inset

 position in image space.
 On top of this, we use a particle filter for tracking the position in image
 space of each detection, and a disambiguation stage to obtain coherent
 hypotheses on the joint position of the markers.
 Finally, we reconstruct the pose using a standard approach to rigid reconstruct
ion.
\end_layout

\begin_layout Standard
We evaluate our method in tracking the pose of a drone during an aggressive
 maneuver (a flip).
 We compare our methods with a traditional approach, using a CMOS camera
 and a feature-based visual odometry method.
 We verify that our method, with a latency of 
\begin_inset Formula $1$
\end_inset


\begin_inset space ~
\end_inset

ms, is able to reacquire tracking instantaneously regardless of the fast
 motion, while the CMOS data is corrupted by motion blur.
 We evaluate the reconstruction accuracy using an OptiTrack system and find
 values that are compatible with the low spatial resolution (128
\begin_inset Formula $\times$
\end_inset

128) of the DVS, which proves to be the current limitation of this approach.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
As this utilization of asynchronous vision is a rather novel approach in
 robotics it would be interesting to use this camera in other navigational
 tasks in future projects, for example for visual SLAM on autonomous ground
 vehicles.
 While small image features as used in SIFT 
\begin_inset CommandInset citation
LatexCommand cite
key "SIFT"

\end_inset

 might be difficult to use due to resolution line feature extraction, as
 describe in 
\begin_inset CommandInset citation
LatexCommand cite
key "LineTracking"

\end_inset

, could be a feasible approach.
 Additionally, as edges are the natural source for DVS events it would have
 an advantage in terms of processing compared to normal cameras.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Software, datasets, and videos illustrating the method are available at
 the website 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
myurl
\end_layout

\end_inset

.
\end_layout

\end_body
\end_document
