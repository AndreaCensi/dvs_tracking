#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass IEEEtran
\begin_preamble
\input{tex/preamble.tex}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\use_refstyle 0
\branch report
\selected 0
\filename_suffix 0
\color #000000
\end_branch
\branch conf
\selected 0
\filename_suffix 0
\color #000000
\end_branch
\branch AC
\selected 1
\filename_suffix 0
\color #682743
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Large
\end_layout

\end_inset

 Low-latency localization by Active LED Markers tracking 
\begin_inset Newline newline
\end_inset

using a Dynamic Vision Sensor
\end_layout

\begin_layout Author
Andrea Censi
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mythanks
\end_layout

\end_inset

, Jonas Strubel, Christian Br
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
"a
\end_layout

\end_inset

ndli, Tobi Delbr
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
"
\end_layout

\end_inset

uck, Davide Scaramuzza
\end_layout

\begin_layout Abstract
At the current state of the art, the agility of an autonomous flying robot
 is limited by the speed of its sensing pipeline, as the relatively high
 latency and low sampling frequency limit the aggressiveness of the control
 strategies that can be implemented.
 The use of Dynamic Vision Sensors (DVS), which encode changes in the visual
 field using an address-event representation, not unlike neuronal spikes
 in biological systems, have a latency that can be measured in the microseconds,
 thus offering the theoretical possibility of creating a sensing pipeline
 whose latency is negligible compared to the dynamics of the platform.
 However, to use this sensor we must rethink the way we interpret visual
 data.
 In this paper we present an approach to low-latency pose tracking using
 a DVS camera and Active Led Markers (ALMs), which are LEDs blinking at
 very high frequency (>1
\begin_inset space ~
\end_inset

KHz).
 The DVS camera time resolution is able to distinguish different frequencies,
 thus avoiding the need for data association.
 We compare the DVS approach to traditional tracking using a CMOS camera,
 and we show that the DVS performance is not affected by fast motion, unlike
 the CMOS camera, which suffers from motion blur.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Itemize

\emph on
Current agile robots use alternative tracking systems.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
DS: one-two paragraphs dissing Raff D'Andrea
\series default
 
\end_layout

\end_deeper
\begin_layout Standard
Currently, the agility of a mobile robot is limited by the speed of the
 sensing pipeline.
 More precisely, 
\begin_inset Quotes eld
\end_inset

speed
\begin_inset Quotes erd
\end_inset

 can be quantified in 
\emph on
observations frequency
\emph default
 and 
\emph on
latency
\emph default
 (
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Discretization-and-latency"

\end_inset

).
 In current state-of-the art autonomous navigation applications (
\series bold
DS: add citations) 
\series default
cameras give observations with frequency of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
xxx
\end_layout

\end_inset

 and the total latency, from acquiring the images to processing them, is
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
xxx
\end_layout

\end_inset

 ms (
\series bold
DS: give reasonable numbers)
\series default
.
 To obtain more agile systems, we need to use faster sensors and low-latency
 processing.
\end_layout

\begin_layout Standard
In this paper, we consider the lowest-latency sensor available, called Dynamic
 Vision Sensor (DVS) and how it can be incorporated in a robotic system
 for the application of pose tracking.
 The main difference of a DVS with respect to a normal CMOS camera is that
 the data is transmitted as a series of 
\emph on
events
\emph default
 (
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:DVS-camera"

\end_inset

).
 Intuitively, the events generated can be interpreted as the sign of the
 derivative of the luminance, but this is just an idealization (
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:The-DVS-camera"

\end_inset

 describes the principles behind the device).
 These events are fired not unlike spikes in a biological visual system,
 as they respond to 
\emph on
change
\emph default
 in the perceived luminance in fact, 
\begin_inset Quotes eld
\end_inset

silicon retina
\begin_inset Quotes erd
\end_inset

 is a nickname for the DVS.
 But the DVS circuits are much faster than a slow neuron: events are generated
 with a latency of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
xxx
\end_layout

\end_inset

 Âµs.
 Therefore, potentially we could obtain sensing pipelines with a negligible
 latency compared to dynamics of the platform.
 Moreover, compared to normal high speed cameras, the data output and thus
 the processing is reduced, as only change is advertised by the camera.
\end_layout

\begin_layout Standard
We are a few years to the goal, however.
 The DVS camera, though currently available commercially, has a few limitations,
 such as the limited resolution (
\begin_inset Formula $128\times128$
\end_inset

 pixels), and it is too heavy to be attached on current agile drones.
 These problems will be solved shortly; here we turn our attention on how
 we could use the data from a DVS for autonomous navigation of flying drones.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement b
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/slides/latency1.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Traditional architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/slides/latency2.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Low latency, event-based architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Discretization-and-latency"

\end_inset

To improve the agility of autonomous flying robots, we need to improve the
 total latency of the sensing pipeline.
 Using a device like a Dynamic Vision Sensor (DVS) we can theoretically
 obtain a sensing pipeline which has microsecond latency.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The application that we show here is localization based on tracking of Active
 LED Markers (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ALMs
\end_layout

\end_inset

).
 These are blinking LEDs at high frequency (
\begin_inset Formula $>1\,\mbox{kHz}$
\end_inset

).
 The DVS is fast enough to be able to estimate the blinking frequency.
 Therefore, we can detect not only the position, but, assuming that each
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ALM
\end_layout

\end_inset

 is given a slightly different blinking frequency, also the identity of
 the markers, thus simplifying data association.
 We envision that this system could be used for inter-robot localization
 for high-speed acrobatic maneuvers, or that, in applications such as rescue
 robotics these markers could be left in the environment to facilitate cooperati
ve mapping.
\end_layout

\begin_layout Standard
It is clear that the way we do computer vision must be completely rethought.
 It is possible to integrate the events of a DVS camera to simulate a regular
 CMOS frame, on which to do standard image processing, however, that is
 not what you would want to do, because accumulating.
 Ideally, to have the lowest latency for the sensing pipeline, one would
 want each single event to be be reflected in a small but instantaneous
 change in the commands given to the actuators.
 Therefore, we really want to consider approaches that possibly use the
 information contained in each single event.
\end_layout

\begin_layout Standard
We have found two main approaches to handling event data.
 So far the DVS events are processed using features (such as lines or points)
 that are tracked through time (
\series bold
CB: add citations/expand)
\series default
.
 This approach works well when the camera is static, because the output
 is very sparse.
 We found out that mounting a DVS camera on a flying robot creates a new
 set of challenges.
 Because of the apparent motion of the environment, the input is not sparse
 anymore.
 Moreover, while in controlled conditions the DVS camera parameters can
 be tuned to obtain the best performance, a robot must be able to work in
 a wider range of environmental conditions and be robust to interferences.
 
\end_layout

\begin_layout Standard
To achieve this robustness we have developed an approach that can sacrifice
 a bit of latency to be more robust to noise and unmodeled phenomena.
 We accumulate the events perceived in thin slices of times corresponding
 to the blinking frequency (
\begin_inset Formula $1$
\end_inset

ms slice for 1
\begin_inset space ~
\end_inset

kHz data).
 This allows to do detection of the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ALMs
\end_layout

\end_inset

 position in image space.
 On top of this, we use a particle filter for tracking the position in image
 space of each detection, and a resolution stage to obtain coherent hypotheses
 on the joint position of the markers.
 Finally we reconstruct the pose using a standard approach to rigid reconstructi
on.
\end_layout

\begin_layout Standard
We evaluate our method in tracking the pose of a drone during an agile maneuver
 (a flip).
 We compare our methods with a traditional CMOS-based approach using PTAM.
 We verify that our method, with a latency of 
\begin_inset Formula $1$
\end_inset


\begin_inset space ~
\end_inset

ms, is able to reacquire tracking instantaneously regardless of the fast
 motion, while the CMOS data is corrupted by motion blur.
 We evaluate the reconstruction accuracy using an OptiTrack system (however,
 at 250 Hz resonse, this is way slower).
 We obtain values that are compatible with the low spatial resolution (128
\begin_inset Formula $\times$
\end_inset

128) of the DVS, which proves to be the current limitation.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
As this utilization of asynchronous vision is a rather novel approach in
 robotics it would be interesting to use this camera in other navigational
 tasks in future projects, for example for visual SLAM on autonomous ground
 vehicles.
 While small image features as used in SIFT 
\begin_inset CommandInset citation
LatexCommand cite
key "SIFT"

\end_inset

 might be difficult to use due to resolution line feature extraction, as
 describe in 
\begin_inset CommandInset citation
LatexCommand cite
key "LineTracking"

\end_inset

, could be a feasible approach.
 Additionally, as edges are the natural source for DVS events it would have
 an advantage in terms of processing compared to normal cameras.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Software, datasets, and animations illustrating the method are available
 at the website 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
xxx
\end_layout

\end_inset


\end_layout

\end_body
\end_document
